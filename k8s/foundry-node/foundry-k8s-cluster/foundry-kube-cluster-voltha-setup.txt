#
# KUBERNETES 3 SERVER CLUSTER BUILD NOTES/SCRIPT
#
#
# Requires three hosts.  8 cpu, 16gb memory, 100GB disk.  
# Ubuntu 16.04 patched and updated and on the network, internet.  
#
# Note the IP and hostnames as will be used later in configs
# Run all of this as root
#
# Each host is referred below by their new alias that will be added to /etc/hosts, 
# master0, master1, and master2.  Their existing hostnames will remain.  Note each of their IP 
# addresses and which will be assigned their respective master role.
#



#
# ALL 3 HOSTS.  master0, master1, and master2
#

# scp from outside host the build package, foundry-k8s-cluster.tgz  
# contains config, yaml, json files for certs, scripts, etc
# upack in ~/source

cd ~/
mkdir source
cd ~/source
git clone https://github.com/donNewtonAlpha/voltha.git
cd ~/
ln -s ~/source/voltha/k8s/foundry-node/foundry-k8s-cluster
cd foundry-k8s-cluster
sudo bash


### including master names
# EDIT hosts-append to reflect the 3 master hosts IP addresses
# TODO: script this
## edit references to master0, master1, and master2 to reflect your the 3 ip addresses for each.
# leave existing hostsnames in /etc/hosts
#
vi hosts-append
cat hosts-append >> /etc/hosts


### install base docker, kubelet, kubadm, etc
#
# get docker-ce
#
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#
# get k8s
#
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

apt update
apt install docker-ce kubelet kubeadm kubectl -y

# Install the AT&T Foundry Atlanta CA cert at the system level.
cat > /usr/local/share/ca-certificates/att-foundry-atlanta-ca.crt << EOF
-----BEGIN CERTIFICATE-----
MIIE8DCCA9igAwIBAgIBADANBgkqhkiG9w0BAQUFADCBsDELMAkGA1UEBhMCVVMx
CzAJBgNVBAgTAkdBMSAwHgYDVQQKFBdBVCZUIEZvdW5kcnkgQXRsYW50YSBDQTEe
MBwGA1UECxMVQ2VydGlmaWNhdGUgQXV0aG9yaXR5MTMwMQYDVQQDFCpBVCZUIEZv
dW5kcnkgQXRsYW50YSBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxHTAbBgkqhkiG9w0B
CQEWDm1qMzU4MEBhdHQuY29tMB4XDTE4MDgxNDE4MjA0OVoXDTI4MDgxMTE4MjA0
OVowgbAxCzAJBgNVBAYTAlVTMQswCQYDVQQIEwJHQTEgMB4GA1UEChQXQVQmVCBG
b3VuZHJ5IEF0bGFudGEgQ0ExHjAcBgNVBAsTFUNlcnRpZmljYXRlIEF1dGhvcml0
eTEzMDEGA1UEAxQqQVQmVCBGb3VuZHJ5IEF0bGFudGEgQ2VydGlmaWNhdGUgQXV0
aG9yaXR5MR0wGwYJKoZIhvcNAQkBFg5tajM1ODBAYXR0LmNvbTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBALLnf1Fxhld4E5/EDAW0h/3ZIb1gN5Zx8ZDc
9Jp3Xpt39few/rO6I2yNDZDBiISPhYTL3MvByAj971bLRbvp4yqMz97D/Fvzrm9E
FPTBye7pfa7BP9dBM1mshQ/7TB6fDx6jfgsCspEuQpIQJMfcy7R911jNbmstetYS
EirnpbyMPx2N3leRcSbmldZtW9sAep9hPqBQZfxCVD5WsYdsmxx6ppwuR4Oogno+
3uVcBosU3s8AezL2tTZ5dtweE5dcfIrbXbE+Cs/9GO3KKxHxFmto/TNo4TqIPVYq
o3yKNAMf9drrmBiJVkhpG+5tTa2/UB5Va/XI9qBKO/8iQw5nLy0CAwEAAaOCAREw
ggENMAwGA1UdEwQFMAMBAf8wHQYDVR0OBBYEFL05Q9KTYs7R+aZ0jukg3EE45KnC
MIHdBgNVHSMEgdUwgdKAFL05Q9KTYs7R+aZ0jukg3EE45KnCoYG2pIGzMIGwMQsw
CQYDVQQGEwJVUzELMAkGA1UECBMCR0ExIDAeBgNVBAoUF0FUJlQgRm91bmRyeSBB
dGxhbnRhIENBMR4wHAYDVQQLExVDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxMzAxBgNV
BAMUKkFUJlQgRm91bmRyeSBBdGxhbnRhIENlcnRpZmljYXRlIEF1dGhvcml0eTEd
MBsGCSqGSIb3DQEJARYObWozNTgwQGF0dC5jb22CAQAwDQYJKoZIhvcNAQEFBQAD
ggEBAJgUgitXd2CFMsWRPLTf2JZbl6LaPYgSVMBc5aBH6xpMfSjQMXFgh134uQzl
iBOd6P9WDneW8N7lABksG/aS7sHTYOisUUlYbCjQdPgo+cm0i4WDXhMN5027TRim
eEo+E+Ge5XEGMTpLUNTN8lncHQvwg7XIYt7NDaQFFDMG25ZUHG2BR7K035fxBLEE
xWx6avSfPkUvlEoVNaiiY1cSr3m1L8GT608zFA6hRqkgAKHtAFNeUfrmlszUBskx
1ea9ij+sr6w92Nluwe5S/uAX8tfAYT+PTvD0+3q2BEwQyVqQhAa+qq8FKfOqxKIX
ufO7tbRNg4POypiXSOabbFfvS+0=
-----END CERTIFICATE-----
EOF

# Apply the addition of the cert.  Should say "1 cert added"
update-ca-certificates

systemctl restart docker
# replace foundry with your local login user
usermod -aG docker foundry

# restart all 3
reboot



##
## Relogin to all 3 hosts, and sudo bash to root again
##


#
# SINGLE HOST. Just from master0
#

##
## prep certs. 
##
cd ~/foundry-k8s-cluster/pki-working/

# manually download cfssl toolkit as its not on apt.  install in /usr/local/bin
#
curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod 755 /usr/local/bin/cfssl*

##
## EDIT kube-apiserver-csr.json to reflect local environment IP and hostnames
## leave references to masterX, kubernetes, and 10.96 ip addresses.  
## Under the section hosts, verify the hostnames master0, master1, and master2 remain, the many kubernetes aliases remain, and 10.96.0.1.
## TODO: script this
## 
vi kube-apiserver-csr.json

# ca for apiserver, kubelet, kubectl
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-kubelet-client-csr.json | cfssljson -bare kube-apiserver-kubelet-client

# ca for front proxy
cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca
cfssl gencert -ca=front-proxy-ca.pem -ca-key=front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare front-proxy-client

# just a simple private/public key for service account
openssl genrsa -out sa.key 2048
openssl rsa -in sa.key -outform PEM -pubout -out sa.pub


# rename so kubeadm below is happy. it expects the files to be named a certain way
mv ca.pem ca.crt
mv ca-key.pem ca.key
mv kube-apiserver.pem apiserver.crt
mv kube-apiserver-key.pem apiserver.key
mv kube-apiserver.csr apiserver.csr
mv kube-apiserver-kubelet-client.pem apiserver-kubelet-client.crt
mv kube-apiserver-kubelet-client-key.pem apiserver-kubelet-client.key
mv kube-apiserver-kubelet-client.csr apiserver-kubelet-client.csr
mv front-proxy-ca-key.pem front-proxy-ca.key
mv front-proxy-ca.pem front-proxy-ca.crt
mv front-proxy-client-key.pem front-proxy-client.key
mv front-proxy-client.pem front-proxy-client.crt

cd ..

# copy all the keys/certs to all 3 hosts from master0
#
scp pki-working/* foundry@master1:~/foundry-k8s-cluster/pki-working/
scp pki-working/* foundry@master2:~/foundry-k8s-cluster/pki-working/



#
# ALL 3 HOSTS.  master0, master1, and master2
#

cd ~/foundry-k8s-cluster
mkdir -p /etc/kubernetes/pki
cp pki-working/* /etc/kubernetes/pki

## Copy to each respective master host its one corresponding master etcd yaml file. 
## master0 should get master0-etcd.yaml,  master1 gets master1-etcd.yaml etc. 
## Do not give any one master multiples etcd yaml files!
##
## TODO: Script this
# master0
cp master0-etcd.yaml /etc/kubernetes/manifests/
# master1
cp master1-etcd.yaml /etc/kubernetes/manifests/
# master2
cp master2-etcd.yaml /etc/kubernetes/manifests/

##
## EDIT kubeadm-config-v2.yaml to reflect local environment IP and hostnames
## Under the section apiServerCertSANs verify the hostnames master0, master1, and master2 remain. 
## Leave 127.0.0.1, replace the other IP addresses with the 3 for your environment.  
## Add in your 3 master's IP addresses and pre-existing hostnames.
## TODO script this
##
vi kubeadm-config-v2.yaml

scp kubeadm-config-v2.yaml foundry@master1:~/foundry-k8s-cluster/
scp kubeadm-config-v2.yaml foundry@master2:~/foundry-k8s-cluster/


## MANUAL KUBEADM METHOD 

kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config-v2.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config-v2.yaml
kubeadm alpha phase kubeconfig all --config kubeadm-config-v2.yaml
kubeadm alpha phase controlplane all --config kubeadm-config-v2.yaml
systemctl start kubelet


### WAIT UNTIL DOCKER LOOKS good.  TBD what good means...
# full docker container list with all columns
docker ps

# 2 second refresh
#watch -n2 docker ps

# list only the command and status columns
docker ps --format '{{.Command}} {{.Status}}'

# wait till all 3 hosts are stable with these 4 containers
# each master should have its own individual etcd, master0, master1, and master2
#
#  "kube-apiserver --en…" Up About a minute
#  "etcd --name=master0…" Up 13 minutes
#  "kube-scheduler --le…" Up 13 minutes
#  "kube-controller-man…" Up 14 minutes


# docker logs <container-id> to investigate
# docker inspect <container-id> to get the containers configuration state.  Look for LogPath to find the container's logging

# Check /var/log/syslog as kubelet will log there its attempts and running the static containers in /etc/kubernetes/manifests



#
# SINGLE HOST. Just from master0
#
kubeadm config upload from-flags 
kubeadm alpha phase bootstrap-token cluster-info /etc/kubernetes/admin.conf
kubeadm alpha phase bootstrap-token node allow-post-csrs 
kubeadm alpha phase bootstrap-token node allow-auto-approve
kubeadm token create --config kubeadm-config-v2.yaml
kubeadm alpha phase addons coredns --config kubeadm-config-v2.yaml
kubeadm alpha phase addons kube-proxy --config kubeadm-config-v2.yaml


mkdir ~/.kube/
cp /etc/kubernetes/admin.conf ~/.kube/config
chown -R foundry:foundry /home/foundry

# verify kubectl
kubectl get pod --all-namespaces -o wide

# scale up dns add node balancing affinity
#kubectl --namespace=kube-system scale deployment coredns --replicas=3
kubectl apply -f coredns-deployment-updated.yaml

# calico networking.
kubectl apply -f calico-rbac-kdd.yaml
kubectl apply -f calico-3.1.3-k8setcd.yaml



#
# ALL 3 HOSTS.  master0, master1, and master2
#

# this was already done on master0, do on the other 2 masters so kubectl works on them if needed
mkdir ~/.kube/
cp /etc/kubernetes/admin.conf ~/.kube/config
chown -R foundry:foundry /home/foundry

# VERIFY
# DO NOT PROCEED UNTIL ALL 3 HOSTS can resolve the dig command below
# verify ALL PODS are running
kubectl get pods --all-namespaces
dig kubernetes.default.svc.cluster.local @10.96.0.10
kubectl run -i --tty network-utils --image=amouat/network-utils --restart=Never --rm=true -- dig kubernetes.default.svc.cluster.local




#
# SINGLE HOST. Just from master0
#
## tools
wget https://github.com/projectcalico/calicoctl/releases/download/v2.0.5/calicoctl
kubectl cp kube-system/master0-etcd-$(hostname):/usr/local/bin/etcdctl .
chmod 755 etcdctl calicoctl
mv etcdctl calicoctl /usr/local/bin/

# verify etcd health. output is good
export ETCDCTL_API=3
etcdctl --endpoints http://127.0.0.1:2379 get /registry --prefix -w fields
etcdctl --endpoints http://127.0.0.1:2379 get /calico --prefix -w fields

# verify calico bgp peering
ETCD_ENDPOINTS=http://127.0.0.1:2379 calicoctl node status



##
## install helm
##
wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz
mkdir helm-unpack
cd helm-unpack/
tar -zxvf ../helm-v2.9.1-linux-amd64.tar.gz
cp linux-amd64/helm /usr/local/bin/

cd ..
kubectl apply -f helm-role.yaml
helm init --service-account tiller
export HELM_HOME=/home/foundry/.helm
helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
helm update


##
## install voltha and dependencies.  finally, why we are here
##
cd ~/source
git clone https://bitbucket.org/onfcord/podconfigs.git
git clone https://gerrit.opencord.org/seba
git clone https://gerrit.opencord.org/helm-charts


# foundry specific values, currently dockers images from the foundry docker-repo
#

cd helm-charts

helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
helm repo add cord https://charts.opencord.org/master
helm repo update
helm repo list

helm install -n cord-kafka incubator/kafka --set replicas=1 --set persistence.enabled=false --set zookeeper.servers=1 --set zookeeper.persistence.enabled=false

helm dep update voltha
helm install -n voltha voltha --set etcd-operator.customResources.createEtcdClusterCRD=false -f ~/foundry-k8s-cluster/att-seba-voltha-values.yaml

helm dep update onos
helm install -n onos-voltha onos -f ~/foundry-k8s-cluster/att-seba-onos-voltha.yaml

# give voltha a couple minutes to install.  verify with kubectl get pods
sleep 120
helm upgrade voltha voltha --set etcd-operator.customResources.createEtcdClusterCRD=true -f ~/foundry-k8s-cluster/att-seba-voltha-values.yaml



##
## Verify whole cluster.  The pods should look very similar to below:
##

kubectl get pods --all-namespaces -o wide

##  NAMESPACE     NAME                                                          READY     STATUS    RESTARTS   AGE       IP              NODE          NOMINATED NODE
##  default       cord-kafka-0                                                  1/1       Running   1          3d        192.168.1.4     nwnn-vgp-01   <none>
##  default       cord-kafka-zookeeper-0                                        1/1       Running   0          3d        192.168.0.6     nwnn-vgp-02   <none>
##  default       cord-kafka-zookeeper-1                                        1/1       Running   0          3d        192.168.2.4     nwnn-vgp-03   <none>
##  default       cord-kafka-zookeeper-2                                        1/1       Running   0          3d        192.168.1.5     nwnn-vgp-01   <none>
##  default       etcd-cluster-0000                                             1/1       Running   0          3d        192.168.1.10    nwnn-vgp-01   <none>
##  default       etcd-cluster-0001                                             1/1       Running   0          3d        192.168.0.11    nwnn-vgp-02   <none>
##  default       etcd-cluster-0002                                             1/1       Running   0          3d        192.168.2.9     nwnn-vgp-03   <none>
##  default       onos-voltha-57569b6f6b-b562l                                  1/1       Running   0          3d        192.168.1.11    nwnn-vgp-01   <none>
##  default       voltha-etcd-operator-etcd-backup-operator-5546b97d78-x64k4    1/1       Running   0          3d        192.168.1.6     nwnn-vgp-01   <none>
##  default       voltha-etcd-operator-etcd-operator-5dfd48694f-5wvwk           1/1       Running   0          3d        192.168.2.5     nwnn-vgp-03   <none>
##  default       voltha-etcd-operator-etcd-restore-operator-7c9ff9c9cd-ps5wd   1/1       Running   0          3d        192.168.0.7     nwnn-vgp-02   <none>
##  kube-system   calico-node-28dzj                                             2/2       Running   0          3d        10.242.11.206   nwnn-vgp-01   <none>
##  kube-system   calico-node-5mkqq                                             2/2       Running   0          3d        10.242.11.207   nwnn-vgp-03   <none>
##  kube-system   calico-node-94wzf                                             2/2       Running   0          3d        10.242.11.209   nwnn-vgp-02   <none>
##  kube-system   coredns-7fb46d7cdd-9srll                                      1/1       Running   0          3d        192.168.2.2     nwnn-vgp-03   <none>
##  kube-system   coredns-7fb46d7cdd-b7krl                                      1/1       Running   0          3d        192.168.1.2     nwnn-vgp-01   <none>
##  kube-system   coredns-7fb46d7cdd-qjg5d                                      1/1       Running   0          3d        192.168.0.5     nwnn-vgp-02   <none>
##  kube-system   kube-apiserver-nwnn-vgp-01                                    1/1       Running   0          3d        10.242.11.206   nwnn-vgp-01   <none>
##  kube-system   kube-apiserver-nwnn-vgp-02                                    1/1       Running   0          3d        10.242.11.209   nwnn-vgp-02   <none>
##  kube-system   kube-apiserver-nwnn-vgp-03                                    1/1       Running   0          3d        10.242.11.207   nwnn-vgp-03   <none>
##  kube-system   kube-controller-manager-nwnn-vgp-01                           1/1       Running   0          3d        10.242.11.206   nwnn-vgp-01   <none>
##  kube-system   kube-controller-manager-nwnn-vgp-02                           1/1       Running   0          3d        10.242.11.209   nwnn-vgp-02   <none>
##  kube-system   kube-controller-manager-nwnn-vgp-03                           1/1       Running   0          3d        10.242.11.207   nwnn-vgp-03   <none>
##  kube-system   kube-proxy-bkh6l                                              1/1       Running   0          3d        10.242.11.207   nwnn-vgp-03   <none>
##  kube-system   kube-proxy-hppp8                                              1/1       Running   0          3d        10.242.11.209   nwnn-vgp-02   <none>
##  kube-system   kube-proxy-tp2tp                                              1/1       Running   0          3d        10.242.11.206   nwnn-vgp-01   <none>
##  kube-system   kube-scheduler-nwnn-vgp-01                                    1/1       Running   0          3d        10.242.11.206   nwnn-vgp-01   <none>
##  kube-system   kube-scheduler-nwnn-vgp-02                                    1/1       Running   0          3d        10.242.11.209   nwnn-vgp-02   <none>
##  kube-system   kube-scheduler-nwnn-vgp-03                                    1/1       Running   0          3d        10.242.11.207   nwnn-vgp-03   <none>
##  kube-system   master0-etcd-nwnn-vgp-01                                      1/1       Running   0          3d        10.242.11.206   nwnn-vgp-01   <none>
##  kube-system   master1-etcd-nwnn-vgp-02                                      1/1       Running   0          3d        10.242.11.209   nwnn-vgp-02   <none>
##  kube-system   master2-etcd-nwnn-vgp-03                                      1/1       Running   0          3d        10.242.11.207   nwnn-vgp-03   <none>
##  kube-system   tiller-deploy-759cb9df9-msmcp                                 1/1       Running   0          3d        192.168.2.3     nwnn-vgp-03   <none>
##  voltha        default-http-backend-846b65fb5f-5bvzq                         1/1       Running   0          3d        192.168.1.9     nwnn-vgp-01   <none>
##  voltha        freeradius-765c9b486c-f2kh2                                   1/1       Running   0          3d        192.168.2.7     nwnn-vgp-03   <none>
##  voltha        netconf-7c6c5756c6-zgvkj                                      1/1       Running   0          3d        192.168.1.7     nwnn-vgp-01   <none>
##  voltha        nginx-ingress-controller-6db99757f7-c8gxr                     1/1       Running   0          3d        192.168.2.6     nwnn-vgp-03   <none>
##  voltha        ofagent-fcff87444-rjhdl                                       1/1       Running   0          3d        192.168.2.10    nwnn-vgp-03   <none>
##  voltha        vcli-69f585cd6d-pscwc                                         1/1       Running   0          3d        192.168.1.8     nwnn-vgp-01   <none>
##  voltha        vcore-0                                                       1/1       Running   0          3d        192.168.2.8     nwnn-vgp-03   <none>
##  voltha        voltha-78749b49d5-xmwxn                                       1/1       Running   3          3d        192.168.0.8     nwnn-vgp-02   <none>



# Properly inject onos config for olt, aaa, and sadis
# for now just run curl shell script to inject needed config.  xos/nem will provide this later
cd ~/foundry-k8s-cluster
./quick-onos-update.sh master0 ~/source/voltha/onos-config/network-cfg.json


#
# verify cluster operation
#

ssh -p 30110 voltha@master0
# username/password voltha/admin
#   devices 
#   table should say empty
#

ssh -p 30115 karaf@master0
# username/password karaf/karaf
#   apps -s -a
#   should list the installed apps



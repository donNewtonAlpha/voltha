#
# KUBERNETES 3 SERVER CLUSTER BUILD NOTES/SCRIPT
#
#
# Requires three hosts.  8 cpu, 8gb memory, 40GB disk.  
# Ubuntu 16.04 patched and updated and on the network, internet.  
#
# Note the IP and hostnames as will be used later in configs
# Run all of this as root
#



#
# ALL 3 HOSTS
#

# scp from outside host the build package, foundry-k8s-cluster.tgz  
# contains config, yaml, json files for certs, scripts, etc
# upack in ~/

sudo bash
tar -zxvf foundry-k8s-cluster.tgz
cd foundry-k8s-cluster


### including master names
# EDIT hosts-append to reflect 3 hosts IP addresses
# TODO: script this
## edit references to masterX to reflect your the 3 ip addresses
vi hosts-append
cat hosts-append >> /etc/hosts


### put kube-dns nameservers using either dhcp or static method.  figure out which
cat dhclient-append >> /etc/dhcp/dhclient.conf
# or
#cat dns-nameservers >> /etc/network/interfaces

## add dns lookup opts
cat resolvconf-base >> /etc/resolvconf/resolv.conf.d/base



### install base docker, kubelet, kubadm, etc
#
# get docker-ce
#
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#
# get k8s
#
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF


sudo apt update
sudo apt install docker-ce kubelet kubeadm kubectl -y


## allow insecure docker repos
sudo cat <<EOF > /etc/docker/daemon.json
{
  "insecure-registries":[ "docker-repo.dev.atl.foundry.att.com:5000" ]
}
EOF

sudo systemctl restart docker
sudo usermod -aG docker foundry

# restart all 3
reboot



#
# SINGLE HOST
#

##
## prep certs. 
##
cd foundry-k8s-cluster/pki-working/

# manually download cfssl toolkit as its not on apt.  install in /usr/local/bin#
curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod 755 /usr/local/bin/cfssl*

##
## EDIT kube-apiserver-csr.json to reflect local environment IP and hostnames
## leave references to masterX, kubernetes, and 10.96 ip addresses.  
## TODO: script this
vi kube-apiserver-csr.json

# ca for apiserver, kubelet, kubectl
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-kubelet-client-csr.json | cfssljson -bare kube-apiserver-kubelet-client

# ca for front proxy
cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca
cfssl gencert -ca=front-proxy-ca.pem -ca-key=front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare front-proxy-client

# just a simple private/public key for service account
openssl genrsa -out sa.key 2048
openssl rsa -in sa.key -outform PEM -pubout -out sa.pub


# rename so kubeadm below is happy. it expects the files to be named a certain way
mv ca.pem ca.crt
mv ca-key.pem ca.key
mv kube-apiserver.pem apiserver.crt
mv kube-apiserver-key.pem apiserver.key
mv kube-apiserver.csr apiserver.csr
mv kube-apiserver-kubelet-client.pem apiserver-kubelet-client.crt
mv kube-apiserver-kubelet-client-key.pem apiserver-kubelet-client.key
mv kube-apiserver-kubelet-client.csr apiserver-kubelet-client.csr
mv front-proxy-ca-key.pem front-proxy-ca.key
mv front-proxy-ca.pem front-proxy-ca.crt
mv front-proxy-client-key.pem front-proxy-client.key
mv front-proxy-client.pem front-proxy-client.crt

cd ..

# copy all the keys/certs to all 3 hosts
scp pki-working/* foundry@master1:~/foundry-k8s-cluster/pki-working/
scp pki-working/* foundry@master2:~/foundry-k8s-cluster/pki-working/



#
# ALL 3 HOSTS
#

mkdir -p /etc/kubernetes/pki
cp pki-working/* /etc/kubernetes/pki

## WHERE X is the master node number 0, 1 ,2, per each host
## TODO: Script this
cp masterX-etcd.yaml /etc/kubernetes/manifests/

##
## EDIT kubeadm-config.yaml to reflect local environment IP and hostnames
## TODO
vi kubeadm-config.yaml

## MANUAL KUBEADM METHOD 

kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml
kubeadm alpha phase controlplane all --config kubeadm-config.yaml
systemctl start kubelet

# OR KUBEADM INIT way...
#kubeadm init --ignore-preflight-errors=ExternalEtcdVersion --config=kubeadm-config.yaml
#kubectl taint nodes --all node-role.kubernetes.io/master-


### WAIT UNTIL DOCKER LOOKS good.  TBD what good means...
docker ps

#watch -n2 docker ps

docker ps --format '{{.Command}} {{.Status}}'

# wait till all 3 hosts are stable with these 5 containers
#
#  "kube-apiserver --en…" Up About a minute
#  "/usr/local/bin/kube…" Up 4 minutes
#  "etcd --name=master0…" Up 13 minutes
#  "kube-scheduler --le…" Up 13 minutes
#  "kube-controller-man…" Up 14 minutes


# docker logs <container-id> to investigate




#
# SINGLE HOST
#
kubeadm config upload from-flags 
kubeadm alpha phase bootstrap-token cluster-info /etc/kubernetes/admin.conf
kubeadm alpha phase bootstrap-token node allow-post-csrs 
kubeadm alpha phase bootstrap-token node allow-auto-approve
kubeadm token create --config kubeadm-config.yaml
kubeadm alpha phase addons kube-dns --config kubeadm-config.yaml
kubeadm alpha phase addons kube-proxy --config kubeadm-config.yaml


mkdir ~/.kube/
cp /etc/kubernetes/admin.conf ~/.kube/config
chown -R foundry:foundry /home/foundry

# verify kubectl
kubectl get pod --all-namespaces

## scale up dns, apply networking, verify basics
kubectl --namespace=kube-system scale deployment kube-dns --replicas=3


# calico networking.
#kubectl apply -f calico-3.1-k8setcd.yaml
kubectl apply -f calico-2.6.8-k8setcd.yaml



#
# ALL 3 HOSTS
#
# this was already done on one, do on the other 2 so kubectl works
mkdir ~/.kube/
cp /etc/kubernetes/admin.conf ~/.kube/config
chown -R foundry:foundry /home/foundry

# VERIFY
# DO NOT PROCEED UNTIL ALL 3 HOSTS can resolve the dig command below
# verify ALL PODS are running
kubectl get pods --all-namespaces
dig @10.96.0.10 kubernetes.default.svc.cluster.local




#
# SINGLE HOST
#
## tools
wget https://github.com/projectcalico/calicoctl/releases/download/v2.0.5/calicoctl
kubectl cp kube-system/master0-etcd-kube-03:/usr/local/bin/etcdctl .
chmod 755 etcdctl calicoctl
mv etcdctl calicoctl /usr/local/bin/

# verify etcd health. output is good
export ETCDCTL_API=2
etcdctl --endpoints http://127.0.0.1:2379 ls /calico
export ETCDCTL_API=3
etcdctl --endpoints http://127.0.0.1:2379 get /registry --prefix -w fields

# verify calico bgp peering
ETCD_ENDPOINTS=http://127.0.0.1:2379 calicoctl node status



##
## install helm
##
wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz
mkdir helm-unpack
cd helm-unpack/
tar -zxvf ../helm-v2.9.1-linux-amd64.tar.gz
cp linux-amd64/helm /usr/local/bin/

cd ..
kubectl apply -f helm-role.yaml
helm init --service-account tiller
export HELM_HOME=/home/foundry/.helm
helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
helm update


##
## install voltha and dependencies.  finally, why we are here
##
git clone https://gerrit.opencord.org/helm-charts

# foundry specific values
cp values.yaml helm-charts/voltha/
## remove freeradius templates
rm voltha/templates/freeradius*

cd helm-charts
helm dep build voltha
helm install -n voltha --set etcd-operator.customResources.createEtcdClusterCRD=false voltha

## THIS WILL TAKE A LONG TIME
# check status with
kubectl get pod --all-namespaces
helm status voltha

## UNDO the install.  due to an etcd-operator bug:  https://github.com/kubernetes/charts/issues/5328
helm delete --purge voltha    

# reinstall really this time
helm install -n voltha voltha
helm install --name onos-voltha -f configs/onos-voltha.yaml onos


# TODO:  properly inject onos config for olt, aaa, and sadis


# verify cluster operation
#
# voltha/admin
# ssh -p 5022 voltha@vcli.voltha.svc.cluster.local
#
# karaf/karaf
# ssh -p 8101 karaf@onos-voltha-ssh.voltha.svc.cluster.local





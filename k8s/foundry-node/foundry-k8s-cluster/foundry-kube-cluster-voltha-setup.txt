#
# KUBERNETES 3 SERVER CLUSTER BUILD NOTES/SCRIPT
#
#
# Requires three hosts.  8 cpu, 16gb memory, 100GB disk.  
# Ubuntu 16.04 patched and updated and on the network, internet.  
#
# Note the IP and hostnames as will be used later in configs
# Run all of this as root
#
# Each host is referred below by their new alias that will be added to /etc/hosts, 
# master0, master1, and master2.  Their existing hostnames will remain.  Note each of their IP 
# addresses and which will be assigned their respective master role.
#



#
# ALL 3 HOSTS.  master0, master1, and master2
#

# scp from outside host the build package, foundry-k8s-cluster.tgz  
# contains config, yaml, json files for certs, scripts, etc
# upack in ~/source

cd ~/
mkdir source
cd ~/source
git clone https://github.com/donNewtonAlpha/voltha.git
cd ~/
ln -s ~/source/voltha/k8s/foundry-node/foundry-k8s-cluster
cd foundry-k8s-cluster
sudo bash


### including master names
# EDIT hosts-append to reflect the 3 master hosts IP addresses
# TODO: script this
## edit references to master0, master1, and master2 to reflect your the 3 ip addresses for each.
# leave existing hostsnames in /etc/hosts
#
vi hosts-append
cat hosts-append >> /etc/hosts


### install base docker, kubelet, kubadm, etc
#
# get docker-ce
#
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#
# get k8s
#
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

apt update
apt install docker-ce kubelet kubeadm kubectl -y

# Install the AT&T Foundry Atlanta CA cert at the system level.
cat > /usr/local/share/ca-certificates/att-foundry-atlanta-ca.crt << EOF
-----BEGIN CERTIFICATE-----
MIIE8DCCA9igAwIBAgIBADANBgkqhkiG9w0BAQUFADCBsDELMAkGA1UEBhMCVVMx
CzAJBgNVBAgTAkdBMSAwHgYDVQQKFBdBVCZUIEZvdW5kcnkgQXRsYW50YSBDQTEe
MBwGA1UECxMVQ2VydGlmaWNhdGUgQXV0aG9yaXR5MTMwMQYDVQQDFCpBVCZUIEZv
dW5kcnkgQXRsYW50YSBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxHTAbBgkqhkiG9w0B
CQEWDm1qMzU4MEBhdHQuY29tMB4XDTE4MDgxNDE4MjA0OVoXDTI4MDgxMTE4MjA0
OVowgbAxCzAJBgNVBAYTAlVTMQswCQYDVQQIEwJHQTEgMB4GA1UEChQXQVQmVCBG
b3VuZHJ5IEF0bGFudGEgQ0ExHjAcBgNVBAsTFUNlcnRpZmljYXRlIEF1dGhvcml0
eTEzMDEGA1UEAxQqQVQmVCBGb3VuZHJ5IEF0bGFudGEgQ2VydGlmaWNhdGUgQXV0
aG9yaXR5MR0wGwYJKoZIhvcNAQkBFg5tajM1ODBAYXR0LmNvbTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBALLnf1Fxhld4E5/EDAW0h/3ZIb1gN5Zx8ZDc
9Jp3Xpt39few/rO6I2yNDZDBiISPhYTL3MvByAj971bLRbvp4yqMz97D/Fvzrm9E
FPTBye7pfa7BP9dBM1mshQ/7TB6fDx6jfgsCspEuQpIQJMfcy7R911jNbmstetYS
EirnpbyMPx2N3leRcSbmldZtW9sAep9hPqBQZfxCVD5WsYdsmxx6ppwuR4Oogno+
3uVcBosU3s8AezL2tTZ5dtweE5dcfIrbXbE+Cs/9GO3KKxHxFmto/TNo4TqIPVYq
o3yKNAMf9drrmBiJVkhpG+5tTa2/UB5Va/XI9qBKO/8iQw5nLy0CAwEAAaOCAREw
ggENMAwGA1UdEwQFMAMBAf8wHQYDVR0OBBYEFL05Q9KTYs7R+aZ0jukg3EE45KnC
MIHdBgNVHSMEgdUwgdKAFL05Q9KTYs7R+aZ0jukg3EE45KnCoYG2pIGzMIGwMQsw
CQYDVQQGEwJVUzELMAkGA1UECBMCR0ExIDAeBgNVBAoUF0FUJlQgRm91bmRyeSBB
dGxhbnRhIENBMR4wHAYDVQQLExVDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxMzAxBgNV
BAMUKkFUJlQgRm91bmRyeSBBdGxhbnRhIENlcnRpZmljYXRlIEF1dGhvcml0eTEd
MBsGCSqGSIb3DQEJARYObWozNTgwQGF0dC5jb22CAQAwDQYJKoZIhvcNAQEFBQAD
ggEBAJgUgitXd2CFMsWRPLTf2JZbl6LaPYgSVMBc5aBH6xpMfSjQMXFgh134uQzl
iBOd6P9WDneW8N7lABksG/aS7sHTYOisUUlYbCjQdPgo+cm0i4WDXhMN5027TRim
eEo+E+Ge5XEGMTpLUNTN8lncHQvwg7XIYt7NDaQFFDMG25ZUHG2BR7K035fxBLEE
xWx6avSfPkUvlEoVNaiiY1cSr3m1L8GT608zFA6hRqkgAKHtAFNeUfrmlszUBskx
1ea9ij+sr6w92Nluwe5S/uAX8tfAYT+PTvD0+3q2BEwQyVqQhAa+qq8FKfOqxKIX
ufO7tbRNg4POypiXSOabbFfvS+0=
-----END CERTIFICATE-----
EOF

# Apply the addition of the cert.  Should say "1 cert added"
update-ca-certificates

systemctl restart docker
# replace foundry with your local login user
usermod -aG docker foundry

# restart all 3
reboot



##
## Relogin to all 3 hosts, and sudo bash to root again
##


#
# SINGLE HOST. Just from master0
#

##
## prep certs. 
##
cd ~/foundry-k8s-cluster/pki-working/

# manually download cfssl toolkit as its not on apt.  install in /usr/local/bin
#
curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod 755 /usr/local/bin/cfssl*

##
## EDIT kube-apiserver-csr.json to reflect local environment IP and hostnames
## leave references to masterX, kubernetes, and 10.96 ip addresses.  
## Under the section hosts, verify the hostnames master0, master1, and master2 remain, the many kubernetes aliases remain, and 10.96.0.1.
## TODO: script this
## 
vi kube-apiserver-csr.json

# ca for apiserver, kubelet, kubectl
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-kubelet-client-csr.json | cfssljson -bare kube-apiserver-kubelet-client

# ca for front proxy
cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca
cfssl gencert -ca=front-proxy-ca.pem -ca-key=front-proxy-ca-key.pem -config=ca-config.json -profile=kubernetes front-proxy-client-csr.json | cfssljson -bare front-proxy-client

# just a simple private/public key for service account
openssl genrsa -out sa.key 2048
openssl rsa -in sa.key -outform PEM -pubout -out sa.pub


# rename so kubeadm below is happy. it expects the files to be named a certain way
mv ca.pem ca.crt
mv ca-key.pem ca.key
mv kube-apiserver.pem apiserver.crt
mv kube-apiserver-key.pem apiserver.key
mv kube-apiserver.csr apiserver.csr
mv kube-apiserver-kubelet-client.pem apiserver-kubelet-client.crt
mv kube-apiserver-kubelet-client-key.pem apiserver-kubelet-client.key
mv kube-apiserver-kubelet-client.csr apiserver-kubelet-client.csr
mv front-proxy-ca-key.pem front-proxy-ca.key
mv front-proxy-ca.pem front-proxy-ca.crt
mv front-proxy-client-key.pem front-proxy-client.key
mv front-proxy-client.pem front-proxy-client.crt

cd ..

# copy all the keys/certs to all 3 hosts from master0
#
scp pki-working/* foundry@master1:~/foundry-k8s-cluster/pki-working/
scp pki-working/* foundry@master2:~/foundry-k8s-cluster/pki-working/



#
# ALL 3 HOSTS.  master0, master1, and master2
#

cd ~/foundry-k8s-cluster
mkdir -p /etc/kubernetes/pki
cp pki-working/* /etc/kubernetes/pki

## Copy to each respective master host its one corresponding master etcd yaml file. 
## master0 should get master0-etcd.yaml,  master1 gets master1-etcd.yaml etc. 
## Do not give any one master multiples etcd yaml files!
##
## TODO: Script this
# master0
cp master0-etcd.yaml /etc/kubernetes/manifests/
# master1
cp master1-etcd.yaml /etc/kubernetes/manifests/
# master2
cp master2-etcd.yaml /etc/kubernetes/manifests/

##
## EDIT kubeadm-config-v2.yaml to reflect local environment IP and hostnames
## Under the section apiServerCertSANs verify the hostnames master0, master1, and master2 remain. 
## Leave 127.0.0.1, replace the other IP addresses with the 3 for your environment.  
## Add in your 3 master's IP addresses and pre-existing hostnames.
## TODO script this
##
vi kubeadm-config-v2.yaml

scp kubeadm-config-v2.yaml foundry@master1:~/foundry-k8s-cluster/
scp kubeadm-config-v2.yaml foundry@master2:~/foundry-k8s-cluster/


## MANUAL KUBEADM METHOD 

kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config-v2.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config-v2.yaml
kubeadm alpha phase kubeconfig all --config kubeadm-config-v2.yaml
kubeadm alpha phase controlplane all --config kubeadm-config-v2.yaml
systemctl start kubelet


### WAIT UNTIL DOCKER LOOKS good.  TBD what good means...
# full docker container list with all columns
docker ps

# 2 second refresh
#watch -n2 docker ps

# list only the command and status columns
docker ps --format '{{.Command}} {{.Status}}'

# wait till all 3 hosts are stable with these 4 containers
# each master should have its own individual etcd, master0, master1, and master2
#
#  "kube-apiserver --en…" Up About a minute
#  "etcd --name=master0…" Up 13 minutes
#  "kube-scheduler --le…" Up 13 minutes
#  "kube-controller-man…" Up 14 minutes


# docker logs <container-id> to investigate
# docker inspect <container-id> to get the containers configuration state.  Look for LogPath to find the container's logging

# Check /var/log/syslog as kubelet will log there its attempts and running the static containers in /etc/kubernetes/manifests



#
# SINGLE HOST. Just from master0
#
kubeadm config upload from-flags 
kubeadm alpha phase bootstrap-token cluster-info /etc/kubernetes/admin.conf
kubeadm alpha phase bootstrap-token node allow-post-csrs 
kubeadm alpha phase bootstrap-token node allow-auto-approve
kubeadm token create --config kubeadm-config-v2.yaml
kubeadm alpha phase addons coredns --config kubeadm-config-v2.yaml
kubeadm alpha phase addons kube-proxy --config kubeadm-config-v2.yaml


mkdir ~/.kube/
cp /etc/kubernetes/admin.conf ~/.kube/config
chown -R foundry:foundry /home/foundry

# verify kubectl
kubectl get pod --all-namespaces -o wide

# scale up dns add node balancing affinity
#kubectl --namespace=kube-system scale deployment coredns --replicas=3
kubectl apply -f coredns-deployment-updated.yaml

# calico networking.
kubectl apply -f calico-rbac-kdd.yaml
kubectl apply -f calico-3.1.3-k8setcd.yaml



#
# ALL 3 HOSTS.  master0, master1, and master2
#

# this was already done on master0, do on the other 2 masters so kubectl works on them if needed
mkdir ~/.kube/
cp /etc/kubernetes/admin.conf ~/.kube/config
chown -R foundry:foundry /home/foundry

# VERIFY
# DO NOT PROCEED UNTIL ALL 3 HOSTS can resolve the dig command below
# verify ALL PODS are running
kubectl get pods --all-namespaces
dig kubernetes.default.svc.cluster.local @10.96.0.10
kubectl run -i --tty network-utils --image=amouat/network-utils --restart=Never --rm=true -- dig kubernetes.default.svc.cluster.local




#
# SINGLE HOST. Just from master0
#
## tools
wget https://github.com/projectcalico/calicoctl/releases/download/v2.0.5/calicoctl
kubectl cp kube-system/master0-etcd-$(hostname):/usr/local/bin/etcdctl .
chmod 755 etcdctl calicoctl
mv etcdctl calicoctl /usr/local/bin/

# verify etcd health. output is good
export ETCDCTL_API=3
etcdctl --endpoints http://127.0.0.1:2379 get /registry --prefix -w fields
etcdctl --endpoints http://127.0.0.1:2379 get /calico --prefix -w fields

# verify calico bgp peering
ETCD_ENDPOINTS=http://127.0.0.1:2379 calicoctl node status



##
## install helm
##
wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz
mkdir helm-unpack
cd helm-unpack/
tar -zxvf ../helm-v2.9.1-linux-amd64.tar.gz
cp linux-amd64/helm /usr/local/bin/

cd ..
kubectl apply -f helm-role.yaml
helm init --service-account tiller
export HELM_HOME=/home/foundry/.helm
helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
helm update


##
## install voltha and dependencies.  finally, why we are here
##
cd ~/source
git clone https://bitbucket.org/onfcord/podconfigs.git
git clone https://gerrit.opencord.org/seba
git clone https://gerrit.opencord.org/helm-charts


# foundry specific values, currently dockers images from the foundry docker-repo
#

cd helm-charts

helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
helm repo add cord https://charts.opencord.org/master
helm repo update
helm repo list

helm install -n cord-kafka incubator/kafka --set replicas=1 --set persistence.enabled=false --set zookeeper.servers=1 --set zookeeper.persistence.enabled=false

helm dep update voltha
helm install -n voltha voltha --set etcd-operator.customResources.createEtcdClusterCRD=false -f ~/foundry-k8s-cluster/att-seba-voltha-values.yaml

helm dep update onos
helm install -n onos-voltha onos -f ~/foundry-k8s-cluster/att-seba-onos-voltha-values.yaml

# give voltha a minute to install.  verify with kubectl get pods
sleep 30
helm upgrade voltha voltha --set etcd-operator.customResources.createEtcdClusterCRD=true -f ~/foundry-k8s-cluster/att-seba-voltha-values.yaml



##
## Verify whole cluster.  The pods should look very similar to below:
##

kubectl get pods --all-namespaces -o wide

##  
##  NAMESPACE     NAME                                                          READY     STATUS    RESTARTS   AGE       IP                NODE
##  default       etcd-cluster-0                                                1/1       Running   0          1m        192.168.252.246   kube-07
##  default       etcd-cluster-1                                                1/1       Running   0          3m        192.168.77.158    kube-08
##  default       etcd-cluster-2                                                1/1       Running   0          2m        192.168.183.26    kube-06
##  default       voltha-kafka-0                                                1/1       Running   2          4m        192.168.252.244   kube-07
##  default       voltha-kafka-1                                                1/1       Running   0          2m        192.168.183.25    kube-06
##  default       voltha-kafka-2                                                1/1       Running   0          1m        192.168.77.160    kube-08
##  default       voltha-zookeeper-0                                            1/1       Running   0          4m        192.168.183.24    kube-06
##  default       voltha-zookeeper-1                                            1/1       Running   0          3m        192.168.252.245   kube-07
##  default       voltha-zookeeper-2                                            1/1       Running   0          3m        192.168.77.159    kube-08
##  kube-system   calico-kube-controllers-5cc6fcf4d9-4tfss                      1/1       Running   1          20m       10.64.1.102       kube-08
##  kube-system   calico-node-7xb6p                                             2/2       Running   2          20m       10.64.1.162       kube-06
##  kube-system   calico-node-8gw7s                                             2/2       Running   2          20m       10.64.1.152       kube-07
##  kube-system   calico-node-xsvgb                                             2/2       Running   2          20m       10.64.1.102       kube-08
##  kube-system   kube-apiserver-kube-06                                        1/1       Running   21         41m       10.64.1.162       kube-06
##  kube-system   kube-apiserver-kube-07                                        1/1       Running   21         1h        10.64.1.152       kube-07
##  kube-system   kube-apiserver-kube-08                                        1/1       Running   21         1h        10.64.1.102       kube-08
##  kube-system   kube-controller-manager-kube-06                               1/1       Running   5          41m       10.64.1.162       kube-06
##  kube-system   kube-controller-manager-kube-07                               1/1       Running   5          55m       10.64.1.152       kube-07
##  kube-system   kube-controller-manager-kube-08                               1/1       Running   6          40m       10.64.1.102       kube-08
##  kube-system   kube-dns-86f4d74b45-hk5ps                                     3/3       Running   3          21m       192.168.77.139    kube-08
##  kube-system   kube-dns-86f4d74b45-p6rqb                                     3/3       Running   3          20m       192.168.252.220   kube-07
##  kube-system   kube-dns-86f4d74b45-pqv8j                                     3/3       Running   3          20m       192.168.183.17    kube-06
##  kube-system   kube-proxy-7bxr6                                              1/1       Running   1          21m       10.64.1.102       kube-08
##  kube-system   kube-proxy-9xr54                                              1/1       Running   1          21m       10.64.1.152       kube-07
##  kube-system   kube-proxy-rstbh                                              1/1       Running   1          21m       10.64.1.162       kube-06
##  kube-system   kube-scheduler-kube-06                                        1/1       Running   6          41m       10.64.1.162       kube-06
##  kube-system   kube-scheduler-kube-07                                        1/1       Running   6          41m       10.64.1.152       kube-07
##  kube-system   kube-scheduler-kube-08                                        1/1       Running   7          56m       10.64.1.102       kube-08
##  kube-system   master0-etcd-kube-06                                          1/1       Running   9          41m       10.64.1.162       kube-06
##  kube-system   master1-etcd-kube-07                                          1/1       Running   8          55m       10.64.1.152       kube-07
##  kube-system   master2-etcd-kube-08                                          1/1       Running   9          41m       10.64.1.102       kube-08
##  kube-system   tiller-deploy-5c688d5f9b-5mqrt                                1/1       Running   1          15m       192.168.252.217   kube-07
##  voltha        dashd-57cdb689cc-b8l9k                                        1/1       Running   3          4m        192.168.77.156    kube-08
##  voltha        default-http-backend-5c6d95c48-c7zkt                          1/1       Running   0          4m        192.168.183.22    kube-06
##  voltha        grafana-5cbd57c584-cm2ll                                      1/1       Running   0          4m        192.168.183.20    kube-06
##  voltha        netconf-6b9f65df49-kc4ck                                      1/1       Running   0          4m        192.168.252.240   kube-07
##  voltha        nginx-ingress-controller-566c84c9fd-fl8wm                     1/1       Running   0          4m        192.168.252.243   kube-07
##  voltha        ofagent-5b9d98975b-wnlzt                                      1/1       Running   0          4m        192.168.183.21    kube-06
##  voltha        onos-voltha-6676c46bbc-z6srd                                  1/1       Running   0          3m        192.168.77.157    kube-08
##  voltha        shovel-688ff789cc-7zltw                                       1/1       Running   2          4m        192.168.183.23    kube-06
##  voltha        vcli-67587c7988-bch42                                         1/1       Running   0          4m        192.168.77.154    kube-08
##  voltha        vcore-7c6b898789-zvfxn                                        1/1       Running   0          4m        192.168.252.241   kube-07
##  voltha        voltha-65cc8fd59f-k66z5                                       1/1       Running   1          4m        192.168.77.155    kube-08



# TODO:  properly inject onos config for olt, aaa, and sadis
# for now just run curl shell script to inject needed config
cd ~/foundry-k8s-cluster
./quick-onos-update.sh master0 ~/source/voltha/onos-config/network-cfg.json


# verify cluster operation
#
# voltha/admin
# ssh -p 5022 voltha@vcli.voltha.svc.cluster.local
#   devices 
#   table should say empty
#
# karaf/karaf
# ssh -p 8101 karaf@onos-voltha-ssh.voltha.svc.cluster.local
#   apps -s -a
#   should list the installed apps





#
# Foundry Single instance voltha on kubernetes (k8s)
# 
# minimum 8vcpu 8gb memory, 40GB disk.  Probably needs higher
#
# basic install of 16.04, patched and updated
#

sudo apt update
sudo apt dist-upgrade
reboot

#
# get docker-ce
#
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#
# get k8s
#
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

sudo apt update
sudo apt install docker-ce kubelet kubeadm kubectl -y

sudo systemctl stop docker
sudo systemctl stop kubelet

# Install the AT&T Foundry Atlanta CA cert at the system level.
sudo bash -c 'cat > /usr/local/share/ca-certificates/att-foundry-atlanta-ca.crt' << EOF
-----BEGIN CERTIFICATE-----
MIIE8DCCA9igAwIBAgIBADANBgkqhkiG9w0BAQUFADCBsDELMAkGA1UEBhMCVVMx
CzAJBgNVBAgTAkdBMSAwHgYDVQQKFBdBVCZUIEZvdW5kcnkgQXRsYW50YSBDQTEe
MBwGA1UECxMVQ2VydGlmaWNhdGUgQXV0aG9yaXR5MTMwMQYDVQQDFCpBVCZUIEZv
dW5kcnkgQXRsYW50YSBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxHTAbBgkqhkiG9w0B
CQEWDm1qMzU4MEBhdHQuY29tMB4XDTE4MDgxNDE4MjA0OVoXDTI4MDgxMTE4MjA0
OVowgbAxCzAJBgNVBAYTAlVTMQswCQYDVQQIEwJHQTEgMB4GA1UEChQXQVQmVCBG
b3VuZHJ5IEF0bGFudGEgQ0ExHjAcBgNVBAsTFUNlcnRpZmljYXRlIEF1dGhvcml0
eTEzMDEGA1UEAxQqQVQmVCBGb3VuZHJ5IEF0bGFudGEgQ2VydGlmaWNhdGUgQXV0
aG9yaXR5MR0wGwYJKoZIhvcNAQkBFg5tajM1ODBAYXR0LmNvbTCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBALLnf1Fxhld4E5/EDAW0h/3ZIb1gN5Zx8ZDc
9Jp3Xpt39few/rO6I2yNDZDBiISPhYTL3MvByAj971bLRbvp4yqMz97D/Fvzrm9E
FPTBye7pfa7BP9dBM1mshQ/7TB6fDx6jfgsCspEuQpIQJMfcy7R911jNbmstetYS
EirnpbyMPx2N3leRcSbmldZtW9sAep9hPqBQZfxCVD5WsYdsmxx6ppwuR4Oogno+
3uVcBosU3s8AezL2tTZ5dtweE5dcfIrbXbE+Cs/9GO3KKxHxFmto/TNo4TqIPVYq
o3yKNAMf9drrmBiJVkhpG+5tTa2/UB5Va/XI9qBKO/8iQw5nLy0CAwEAAaOCAREw
ggENMAwGA1UdEwQFMAMBAf8wHQYDVR0OBBYEFL05Q9KTYs7R+aZ0jukg3EE45KnC
MIHdBgNVHSMEgdUwgdKAFL05Q9KTYs7R+aZ0jukg3EE45KnCoYG2pIGzMIGwMQsw
CQYDVQQGEwJVUzELMAkGA1UECBMCR0ExIDAeBgNVBAoUF0FUJlQgRm91bmRyeSBB
dGxhbnRhIENBMR4wHAYDVQQLExVDZXJ0aWZpY2F0ZSBBdXRob3JpdHkxMzAxBgNV
BAMUKkFUJlQgRm91bmRyeSBBdGxhbnRhIENlcnRpZmljYXRlIEF1dGhvcml0eTEd
MBsGCSqGSIb3DQEJARYObWozNTgwQGF0dC5jb22CAQAwDQYJKoZIhvcNAQEFBQAD
ggEBAJgUgitXd2CFMsWRPLTf2JZbl6LaPYgSVMBc5aBH6xpMfSjQMXFgh134uQzl
iBOd6P9WDneW8N7lABksG/aS7sHTYOisUUlYbCjQdPgo+cm0i4WDXhMN5027TRim
eEo+E+Ge5XEGMTpLUNTN8lncHQvwg7XIYt7NDaQFFDMG25ZUHG2BR7K035fxBLEE
xWx6avSfPkUvlEoVNaiiY1cSr3m1L8GT608zFA6hRqkgAKHtAFNeUfrmlszUBskx
1ea9ij+sr6w92Nluwe5S/uAX8tfAYT+PTvD0+3q2BEwQyVqQhAa+qq8FKfOqxKIX
ufO7tbRNg4POypiXSOabbFfvS+0=
-----END CERTIFICATE-----
EOF

# Apply the addition of the cert.  Should say "1 cert added"
sudo update-ca-certificates

# Test local CA approves https server cert
# Ask Matt Jeanneret for credentials
curl -u #<provided-username# -v https://docker-repo.dev.atl.foundry.att.com:5000/v2/_catalog

sudo systemctl start docker
sudo systemctl start kubelet
sudo systemctl enable docker
sudo systemctl enable kubelet
sudo systemctl status docker
sudo systemctl status kubelet
# loaded is ok for kubelet. it will exit-loop until a k8s cluster is setup

sudo usermod -aG docker <non-root-user>

# Install the docker repo credentials as your non-root/development user.  The user you "docker pull" as
# Ask Matt Jeanneret for credentials
docker login docker-repo.dev.atl.foundry.att.com:5000

# Test docker pull
docker pull docker-repo.dev.atl.foundry.att.com:5000/ubuntu:16.04

## Add the k8s resolver into the top of the hosts resolver search list
## the actual resolver will be installed below.  
## Also lower the timeout and attempts so the system default resolver is attempted quicker
## in the event the k8s resolver isnt responding
sudo sh -c 'echo "nameserver 10.96.0.10" >> /etc/resolvconf/resolv.conf.d/head'
sudo sh -c 'echo "options ndots:5 timeout:1 attempts:1" >> /etc/resolvconf/resolv.conf.d/base'


reboot

## REBOOT ##




### stage local persistent directories

sudo mkdir -p /var/lib/voltha-runtime/etcd
sudo mkdir -p /var/lib/voltha-runtime/consul/data
sudo mkdir -p /var/lib/voltha-runtime/consul/config
sudo mkdir -p /var/lib/voltha-runtime/fluentd
sudo mkdir -p /var/lib/voltha-runtime/kafka
sudo mkdir -p /var/lib/voltha-runtime/zookeeper/data
sudo mkdir -p /var/lib/voltha-runtime/zookeeper/datalog
sudo mkdir -p /var/lib/voltha-runtime/onos/config
sudo chown -R $(id -u):$(id -g) /var/lib/voltha-runtime


## intialize base k8s environment

# kubeadm doesnt like swap
sudo swapoff -a

sudo kubeadm init
### COPY RESULTS into ~/kubeadm-setup.txt

# make non-root user able to use kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

## VERIFY BASICS

kubectl get all --all-namespaces


### NOT SURE THIS IS NEEDED. i skipped most recently... leave off for now
kubectl -n kube-system get ds -l 'k8s-app==kube-proxy' -o json \
| jq '.items[0].spec.template.spec.containers[0].command |= .+ ["--proxy-mode=userspace"]' \
| kubectl apply -f - && kubectl -n kube-system delete pods -l 'k8s-app==kube-proxy'

# allow master node to run deployments

kubectl taint nodes --all node-role.kubernetes.io/master-

## git clone voltha
mkdir -p ~/source
cd ~/source
git clone https://github.com/donNewtonAlpha/voltha.git
cd ~/source/voltha/k8s/

# setup container networking.
kubectl apply -f calico-1.6.yml 



### STOPPING BEFORE VOLTHA SPECIFICS.  CHECK ENVIRONMENT HEALTH ####

kubectl get pods --all-namespaces
kubectl get all --all-namespaces

# verify calico networking a dns
dig +short @10.96.0.10 calico-etcd.kube-system.svc.cluster.local
dig +short @10.96.0.10 kubernetes.default.svc.cluster.local
dig +short @10.96.0.10 SRV _https._tcp.kubernetes.default.svc.cluster.local




### VOLTHA BEGINS ###

kubectl apply -f namespace.yml 
kubectl apply -f ingress/

# BASE COMPONENTS

## these files have been edited to reflect persistent environment

kubectl apply -f foundry-node/zookeeper_persist.yml
kubectl apply -f foundry-node/kafka_persist.yml
kubectl apply -f foundry-node/etcd_persist.yml
kubectl apply -f foundry-node/fluentd.yml

# verify pods startup, describe and get logs on select ones

kubectl get pod -n voltha
kubectl describe pod kafka-0 -n voltha
kubectl logs kafka-0 -n voltha
kubectl describe pod etcd-0 -n voltha
kubectl logs etcd-0 -n voltha

## verify mount binds are writing data

ls -atlrR /var/lib/voltha-runtime/



### VOLTHA CORE AND ACCESSORY CONTAINERS ####

kubectl apply -f foundry-node/vcore_for_etcd_repo.yml
kubectl apply -f foundry-node/ofagent_repo.yml 
kubectl apply -f foundry-node/envoy_for_etcd_repo.yml 
kubectl apply -f foundry-node/vcli_repo.yml 
kubectl apply -f foundry-node/netconf_repo.yml 
kubectl apply -f foundry-node/stats_repo.yml

## edit onos network config to reflect your OLT hardware.  Otherwise copy as-is
cp ~/source/voltha/onos-config/network-cfg.json /var/lib/voltha-runtime/onos/config/

kubectl apply -f foundry-node/onos_repo.yml 


# verify all pods
kubectl get pods -n voltha

## check health, vcore is the big one
kubectl get pods -n voltha   # get pod name of choice
kubectl describe pod <pod-name> -n voltha
kubectl log <pod-name> -n voltha -f --tail=20
kubectl exec -ti <pod-name> -n voltha -- /bin/sh
#   exit shell back to host



# get svc ip and connect cli
kubectl get svc -n voltha  # note the vcli clusterIP, and the onos clusterIP

# voltha, password admin
ssh -p 5022 voltha@<vcli-cluster-IP>
## run "devices" to verify connectivity to etcd.  you should see "table empty"
## type quit to exit voltha shell

# onos, password karaf
ssh -p 8101 karaf@<onos-cluster-IP>
## run "netcfg" to verify 
## type logout to exit onos shell


### VOLTHA CORE DONE ###

### USEFUL SCRIPTS AND ALIASES ###

Add this to your .bashrc file

alias purge='~/source/voltha/k8s/foundry-node/scripts/etcd-clean.sh purge'
alias vcli='ssh -p 5022 -o StrictHostKeyChecking=no voltha@vcli.voltha.svc.cluster.local'
alias onos='ssh-keygen -f "/home/foundry/.ssh/known_hosts" -R [onos.voltha.svc.cluster.local]:8101; ssh -p 8101 -o StrictHostKeyChecking=no karaf@onos.voltha.svc.cluster.local'


kgp() {
  kubectl get pods --all-namespaces
}

kvl() {
  pod=$(kubectl get pods -n voltha |grep vcore | awk '{print $1}')
  kubectl logs $pod -n voltha $@
}

kol() {
  pod=$(kubectl get pods -n voltha |grep onos | awk '{print $1}')
  kubectl logs $pod -n voltha $@
}

# purge clear etcd
# vcli access voltha cli
# onos access onos cli
# kgp get kubernetes pods
# kvl get vcore's log
# kol get onos' log

### USEFUL SCRIPTS AND ALIASES DONE ###

### PONSIM IF NEEDED ###

## deploy ponsim if needed.  WARNING USES LOTS OF CPU AND DISK USAGE FOR LOGGING. WILL CAUSE EVICTED CONTAINERS

kubectl apply -f foundry-node/olt_repo.yml 
kubectl apply -f foundry-node/onu_repo.yml 


## delete ponsim (or other resources)

kubectl delete -f foundry-node/olt_repo.yml
kubectl delete -f foundry-node/onu_repo.yml 

### PONSIM DONE ###

### TO BUILD CONTAINERS ###

sudo apt-get update
sudo apt-get install build-essential virtualenv python-dev python-pip python libssl-dev libpcap-dev python-netifaces \
python-virtualenv python-urllib3 python-nose python-flake8 python-scapy

cd ~/source/voltha

. env.sh
make install-protoc
make build

# Next time if you just want to rebuild the voltha container
. env.sh
make build voltha




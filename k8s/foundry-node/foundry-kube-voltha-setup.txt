#
# Foundry Single instance voltha on kubernetes (k8s)
# 
# minimum 8vcpu 8gb memory, 40GB disk.  Probably needs higher
#
# basic install of 16.04, patched and updated
#

sudo apt update
sudo apt dist-upgrade
reboot

#
# get docker-ce
#
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#
# get k8s
#
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

sudo apt update
sudo apt install docker-ce kubelet kubeadm kubectl -y


sudo systemctl start docker
sudo systemctl start kubelet
sudo systemctl enable docker
sudo systemctl enable kubelet
sudo systemctl status docker
sudo systemctl status kubelet
# loaded is ok for kubelet. it will exit-loop until a k8s cluster is setup


## allow insecure docker repos
sudo cat <<EOF > /etc/docker/daemon.json
{
  "insecure-registries":[ "docker-repo.dev.atl.foundry.att.com:5000" ]
}
EOF

sudo systemctl restart docker
sudo usermod -aG docker <non-root-user>

reboot

## REBOOT ##




### stage local persistent directories

sudo mkdir -p /var/lib/voltha-runtime/etcd
sudo mkdir -p /var/lib/voltha-runtime/consul/data
sudo mkdir -p /var/lib/voltha-runtime/consul/config
sudo mkdir -p /var/lib/voltha-runtime/fluentd
sudo mkdir -p /var/lib/voltha-runtime/kafka
sudo mkdir -p /var/lib/voltha-runtime/zookeeper/data
sudo mkdir -p /var/lib/voltha-runtime/zookeeper/datalog
sudo mkdir -p /var/lib/voltha-runtime/grafana/data
sudo mkdir -p /var/lib/voltha-runtime/onos/config
sudo chown -R $(id -u):$(id -g) /var/lib/voltha-runtime


## intialize base k8s environment

# kubeadm doesnt like swap
sudo swapoff -a

sudo kubeadm init
### COPY RESULTS into ~/kubeadm-setup.txt

# make non-root user able to use kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

## VERIFY BASICS

kubectl get all --all-namespaces


### NOT SURE THIS IS NEEDED. i skipped most recently... leave off for now
kubectl -n kube-system get ds -l 'k8s-app==kube-proxy' -o json \
| jq '.items[0].spec.template.spec.containers[0].command |= .+ ["--proxy-mode=userspace"]' \
| kubectl apply -f - && kubectl -n kube-system delete pods -l 'k8s-app==kube-proxy'

# allow master node to run deployments

kubectl taint nodes --all node-role.kubernetes.io/master-

## git clone voltha
mkdir -p ~/source
cd ~/source
git clone https://github.com/donNewtonAlpha/voltha.git
cd ~/source/voltha/k8s/

# setup container networking.
kubectl apply -f calico-1.6.yml 



### STOPPING BEFORE VOLTHA SPECIFICS.  CHECK ENVIRONMENT HEALTH ####

kubectl get pods --all-namespaces
kubectl get all --all-namespaces

# verify calico networking a dns
dig +short @10.96.0.10 calico-etcd.kube-system.svc.cluster.local
dig +short @10.96.0.10 kubernetes.default.svc.cluster.local
dig +short @10.96.0.10 SRV _https._tcp.kubernetes.default.svc.cluster.local




### VOLTHA BEGINS ###

kubectl apply -f namespace.yml 
kubectl apply -f ingress/

# BASE COMPONENTS

## these files have been edited to reflect persistent environment

kubectl apply -f foundry-node/zookeeper_persist.yml
kubectl apply -f foundry-node/kafka_persist.yml
kubectl apply -f foundry-node/etcd_persist.yml
kubectl apply -f foundry-node/fluentd.yml

# verify pods startup, describe and get logs on select ones

kubectl get pod -n voltha
kubectl describe pod kafka-0 -n voltha
kubectl logs kafka-0 -n voltha
kubectl describe pod etcd-0 -n voltha
kubectl logs etcd-0 -n voltha

## verify mount binds are writing data

ls -atlrR /var/lib/voltha-runtime/



### VOLTHA CORE AND ACCESSORY CONTAINERS ####

kubectl apply -f foundry-node/vcore_for_etcd_repo.yml
kubectl apply -f foundry-node/ofagent_repo.yml 
kubectl apply -f foundry-node/envoy_for_etcd_repo.yml 
kubectl apply -f foundry-node/vcli_repo.yml 
kubectl apply -f foundry-node/netconf_repo.yml 
kubectl apply -f foundry-node/grafana_persist.yml 
kubectl apply -f foundry-node/stats_repo.yml 

## edit onos network config to reflect your OLT hardware.  Otherwise copy as-is
cp ~/source/voltha/onos-config/network-cfg.json /var/lib/voltha-runtime/onos/config/

kubectl apply -f foundry-node/onos_repo.yml 


# verify all pods
kubectl get pods -n voltha

## check health, vcore is the big one
kubectl get pods -n voltha   # get pod name of choice
kubectl describe pod <pod-name> -n voltha
kubectl log <pod-name> -n voltha -f --tail=20
kubectl exec -ti <pod-name> -n voltha -- /bin/sh
#   exit shell back to host



# get svc ip and connect cli
kubectl get svc -n voltha  # note the vcli clusterIP, and the onos clusterIP

# voltha, password admin
ssh -p 5022 voltha@<vcli-cluster-IP>
## run "devices" to verify connectivity to etcd.  you should see "table empty"
## type quit to exit voltha shell

# onos, password karaf
ssh -p 8101 karaf@<onos-cluster-IP>
## run "netcfg" to verify 
## type logout to exit onos shell


### VOLTHA CORE DONE ### 

### PONSIM IF NEEDED ###

## deploy ponsim if needed.  WARNING USES LOTS OF CPU AND DISK USAGE FOR LOGGING. WILL CAUSE EVICTED CONTAINERS

kubectl apply -f foundry-node/olt_repo.yml 
kubectl apply -f foundry-node/onu_repo.yml 


## delete ponsim (or other resources)

kubectl delete -f foundry-node/olt_repo.yml
kubectl delete -f foundry-node/onu_repo.yml 

### PONSIM DONE ###

### TO BUILD CONTAINERS ###

sudo apt-get update
sudo apt-get install build-essential virtualenv python-dev python-pip python libssl-dev libpcap-dev python-netifaces \
python-virtualenv python-urllib3 python-nose python-flake8 python-scapy

cd ~/source/voltha

. env.sh
make install-protoc
make build

# Next time if you just want to rebuild the voltha container
. env.sh
make build voltha



